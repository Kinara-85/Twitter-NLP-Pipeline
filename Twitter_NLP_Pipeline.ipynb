{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Sentiment Analysis and Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "iYbnzozLAgDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate"
      ],
      "metadata": {
        "id": "Gs2megoLwErX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R92P0dkvyCV"
      },
      "outputs": [],
      "source": [
        "api_key = 'xxxxxxxxxxxxxxxxx'\n",
        "api_key_secret = 'xxxxxxxxxxxxxxxxxx'\n",
        "acess_token = 'xxxxxxxxxxxxxxxxxx'\n",
        "acess_token_secret = 'xxxxxxxxxxxxxxxxxxxxx'\n",
        "\n",
        "#Authenticate\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting code"
      ],
      "metadata": {
        "id": "T813Y-PQw_IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import csv\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define keywords for scraping tweets\n",
        "keywords = ['keyword']\n",
        "\n",
        "# Define the time period (last 14 days)\n",
        "since_date = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Define the number of tweets to fetch per keyword\n",
        "num_tweets = 10\n",
        "\n",
        "# List to store scraped tweets\n",
        "tweets = []\n",
        "\n",
        "# Perform the search query\n",
        "for keyword in keywords:\n",
        "    query = f\"{keyword} since:{since_date} -filter:retweets\"\n",
        "    searched_tweets = tweepy.Cursor(\n",
        "        api.search_tweets, q=query, lang=\"en\", tweet_mode=\"extended\"\n",
        "    ).items(num_tweets)\n",
        "\n",
        "    for tweet in searched_tweets:\n",
        "        tweets.append(tweet)\n",
        "\n",
        "# Save the tweets to a CSV file\n",
        "with open(\"EOS.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"tweet\", \"created_at\"])\n",
        "\n",
        "    for tweet in tweets:\n",
        "        writer.writerow([tweet.full_text.replace('\\n', ' '), tweet.created_at])\n"
      ],
      "metadata": {
        "id": "nyH8uLmCwHF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP labelling"
      ],
      "metadata": {
        "id": "uBm5aY0ixgab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Set the path for the input CSV file\n",
        "input_file = \"/content/.csv\"  # Path to the CSV\n",
        "\n",
        "# Load the input CSV file\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Create a new column for sentiment labels\n",
        "def get_sentiment_label(tweet):\n",
        "    blob = TextBlob(tweet)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.4:\n",
        "        return 'Positive'\n",
        "    elif polarity > 0:\n",
        "        return 'Moderately Positive'\n",
        "    elif polarity == 0:\n",
        "        return 'Neutral'\n",
        "    elif polarity > -0.4:\n",
        "        return 'Moderately Negative'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "df['Sentiment'] = df['tweet'].apply(get_sentiment_label)\n",
        "\n",
        "# Add an ID column\n",
        "df['id'] = df.index + 1\n",
        "\n",
        "# Reorder the columns\n",
        "df = df[['id', 'created_at', 'tweet', 'Sentiment']]\n",
        "\n",
        "# Display all columns side by side\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Save the new CSV file\n",
        "output_file = input_file.replace(\".csv\", \"_labeled.csv\")\n",
        "df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "Mw_4Pc2xxabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "count labels"
      ],
      "metadata": {
        "id": "fpsJ9_Lfxszc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the labelled tweets CSV file into a DataFrame\n",
        "df_tweets = pd.read_csv('/content/Rose_labeled.csv')\n",
        "\n",
        "# count the number of positive, negative, and neutral tweets\n",
        "counts = df_tweets['Sentiment'].value_counts()\n",
        "\n",
        "# print the counts\n",
        "print(counts)"
      ],
      "metadata": {
        "id": "VXKh6ltwxuNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Shortwords"
      ],
      "metadata": {
        "id": "rS2TGLt4xvAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df_tweets = pd.read_csv('/content/EOS_labeled.csv')  # Input path\n",
        "\n",
        "# Section: Short Words Mapping\n",
        "# Define a dictionary to map short words to their full form\n",
        "short_words = {\n",
        "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"ya\": \"you all\", \"121\": \"one to one\",\n",
        "    \"a/s/l\": \"age, sex, location\", \"adn\": \"any day now\", \"afaik\": \"as far as I know\",\n",
        "    \"afk\": \"away from keyboard\", \"aight\": \"alright\", \"alol\": \"actually laughing out loud\",\n",
        "    \"b4\": \"before\", \"b4n\": \"bye for now\", \"bak\": \"back at the keyboard\",\n",
        "    \"bf\": \"boyfriend\", \"bff\": \"best friends forever\", \"bfn\": \"bye for now\",\n",
        "    \"bg\": \"big grin\", \"bta\": \"but then again\", \"btw\": \"by the way\"\n",
        "}\n",
        "\n",
        "# Section: Replace Short Words\n",
        "def replace_short_words(tweet):\n",
        "    \"\"\"Replace short words in a tweet with their full form.\"\"\"\n",
        "    if not isinstance(tweet, str):\n",
        "        return tweet\n",
        "    words = tweet.split()\n",
        "    words = [short_words[word] if word in short_words else word for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply the function to the \"tweet\" column\n",
        "df_tweets[\"tweet\"] = df_tweets[\"tweet\"].apply(replace_short_words)\n",
        "\n",
        "# Section: Save Cleaned Dataset\n",
        "# df_tweets.to_csv('short_word_labelled_tweets.csv', index=False)  # Save as CSV\n"
      ],
      "metadata": {
        "id": "Wd5ThG6ix28C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing Contraction"
      ],
      "metadata": {
        "id": "KJnktJvj-OGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define apostrophe dictionary for contractions\n",
        "apostrophes = {\n",
        "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he's\": \"he is\",\n",
        "    \"i'm\": \"I am\", \"isn't\": \"is not\", \"it's\": \"it is\", \"let's\": \"let us\", \"she's\": \"she is\",\n",
        "    \"that's\": \"that is\", \"there's\": \"there is\", \"they're\": \"they are\", \"we're\": \"we are\",\n",
        "    \"weren't\": \"were not\", \"what's\": \"what is\", \"where's\": \"where is\", \"who's\": \"who is\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Replace contractions with full words\n",
        "def replace_apostrophes(tweet):\n",
        "    if isinstance(tweet, str):\n",
        "        for old, new in apostrophes.items():\n",
        "            tweet = tweet.replace(old, new)\n",
        "    return tweet\n",
        "\n",
        "df_tweets['tweet'] = df_tweets['tweet'].apply(replace_apostrophes)\n"
      ],
      "metadata": {
        "id": "sFrirdD1-QzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Tweet Text"
      ],
      "metadata": {
        "id": "BIt-P3Vn-XKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove RT, special characters, and convert to lowercase\n",
        "def clean_tweet(tweet):\n",
        "    tweet = re.sub(r'RT @\\w+: ', '', tweet)  # Remove retweets\n",
        "    tweet = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', tweet)  # Remove mentions, links, and special characters\n",
        "    return tweet.lower()\n",
        "\n",
        "df_tweets['tweet'] = df_tweets['tweet'].apply(clean_tweet)\n",
        "\n",
        "# Remove duplicate tweets\n",
        "df_tweets.drop_duplicates(subset='tweet', inplace=True)"
      ],
      "metadata": {
        "id": "kVxLhCNb-Zsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spell Correction"
      ],
      "metadata": {
        "id": "KZQb1TP4-alZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "def correct_misspelled_words(tweet):\n",
        "    words = tweet.split()\n",
        "    corrected_words = [spell.correction(word) if spell.unknown([word]) else word for word in words]\n",
        "    return ' '.join(filter(None, corrected_words))\n",
        "\n",
        "df_tweets['tweet'] = df_tweets['tweet'].apply(correct_misspelled_words)\n"
      ],
      "metadata": {
        "id": "WSFPkWXf-fK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword Removal"
      ],
      "metadata": {
        "id": "m34EcCHh-l1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "def remove_stop_words(tweet):\n",
        "    return ' '.join([word for word in tweet.split() if word not in stopwords])\n",
        "\n",
        "df_tweets['tweet'] = df_tweets['tweet'].apply(remove_stop_words)"
      ],
      "metadata": {
        "id": "cft51iez-npT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Lemmatization"
      ],
      "metadata": {
        "id": "exDaIbjp-onK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df_tweets['tokens'] = df_tweets['tweet'].apply(word_tokenize)\n",
        "df_tweets['lemmatized'] = df_tweets['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n"
      ],
      "metadata": {
        "id": "g30m-_2k-v96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tweets.to_csv('EOS_tweets.csv', index=False)\n"
      ],
      "metadata": {
        "id": "dQwQMWdV-zCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}